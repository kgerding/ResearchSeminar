factor = 'factor',
loc_zip  = 'numeric',
loc_county = 'factor',
age = 'numeric',
flag_fireplace = 'numeric',
num_tax_building = 'numeric',
num_tax_total = 'numeric',
num_tax_land = 'numeric',
num_unit = 'numeric',
quality_factor = 'factor',
heating_factor = 'factor',
prop_living = 'numeric',
build_land_prop = 'numeric')
for (i in colnames(house_only16_mv)) {
if (colClasses[i][[1]] == 'numeric') {
house_only16_mv$i <- as.numeric(house_only16_mv$i)
} else if (colClasses[[i]] == 'factor') {
house_only16_mv$i <- as.factor(house_only16_mv$i)}
}
str(house_only16_mv2)
str(house_only16_mv)
house_only16_mv$quality_factor <- as.factor(house_only16_mv$quality_factor)
str(house_only16_mv)
for (i in colnames(house_only16_mv)) {
print(i)}
for (i in colnames(house_only16_mv)) {
print(house_only16_mv$i)}
for (i in colnames(house_only16_mv)) {
print(i)
print(house_only16_mv$i)}
for (i in colnames(house_only16_mv)) {
print(i)
print(head(house_only16_mv$i))}
i in colnames(house_only16_mv
colnames(house_only16_mv)
house_only16_mv <- fread('./Data/house_only16_mv.csv', drop = 'V1')
View(house_only16_mv)
for (i in colnames(house_only16_mv)) {
print(i)
print(head(house_only16_mv$i))}
house_only16_mv2 <- data.frame(house_only16_mv2)
for (i in colnames(house_only16_mv)) {
print(i)
print(head(house_only16_mv[,i]))}
for (i in colnames(house_only16_mv)) {
print(i[[1]])
print(head(house_only16_mv[,i]))}
i = 'age'
house_only16_mv$i
house_only16_mv$age
house_only16_mv[, i]
house_only16_mv[i]
house_only16_mv[[i]]
for (i in colnames(house_only16_mv)) {
print(i[[1]])
print(head(house_only16_mv[[i]]))}
for (i in colnames(house_only16_mv)) {
if (colClasses[i][[1]] == 'numeric') {
house_only16_mv[[i]] <- as.numeric(house_only16_mv[[i]])
} else if (colClasses[[i]] == 'factor') {
house_only16_mv[[i]] <- as.factor(house_only16_mv[[i]])}
}
str(house_only16_mv)
house_only16_mv <- fread('./Data/house_only16_mv.csv', drop = 'V1')
str(house_only16_mv2)
# Step 6: save the dataframe ------------------------------------------
write.csv(house_only16_mv, './Data/house_only16_mv')
# Step 6: save the dataframe ------------------------------------------
write.csv(house_only16_mv, './Data/house_only16_mv.csv')
house_only16_mv <- fread('./Data/house_only16_mv.csv', drop = 'V1')
colClasses = c(id_parcel = 'numeric',
num_bathroom = 'numeric',
num_bedroom = 'numeric',
area_live_finished = 'numeric',
flag_tub_or_spa = 'numeric',
loc_latitude = 'numeric',
loc_longitude = 'numeric',
area_lot = 'numeric',
factor = 'factor',
loc_zip  = 'numeric',
loc_county = 'factor',
age = 'numeric',
flag_fireplace = 'numeric',
num_tax_building = 'numeric',
num_tax_total = 'numeric',
num_tax_land = 'numeric',
num_unit = 'numeric',
quality_factor = 'factor',
heating_factor = 'factor',
prop_living = 'numeric',
build_land_prop = 'numeric')
str(house_only16_mv2)
for (i in colnames(house_only16_mv)) {
if (colClasses[i][[1]] == 'numeric') {
house_only16_mv[[i]] <- as.numeric(house_only16_mv[[i]])
} else if (colClasses[[i]] == 'factor') {
house_only16_mv[[i]] <- as.factor(house_only16_mv[[i]])
} else if (colClasses[[i]] == 'numeric_character'){
house_only16_mv[[i]] <- as.numeric(sub(",", ".", house_only16_mv[[i]], fixed = TRUE))
}
}
str(house_only16_mv)
house_only16_mv <- fread('./Data/house_only16_mv.csv', drop = 'V1')
house_only16_mv[[prop_living]] <- as.numeric(sub(",", ".", house_only16_mv[[prop_living]], fixed = TRUE))
View(house_only16_mv)
house_only16_mv$prop_living <- as.numeric(sub(",", ".", house_only16_mv$prop_living, fixed = TRUE))
View(house_only16_mv)
i = 'prop_living'
house_only16_mv[[i]] <- as.numeric(sub(",", ".",  house_only16_mv[[i]], fixed = TRUE))
house_only16_mv <- fread('./Data/house_only16_mv.csv', drop = 'V1')
colClasses = c(id_parcel = 'numeric',
num_bathroom = 'numeric',
num_bedroom = 'numeric',
area_live_finished = 'numeric',
flag_tub_or_spa = 'numeric',
loc_latitude = 'numeric',
loc_longitude = 'numeric',
area_lot = 'numeric',
factor = 'factor',
loc_zip  = 'numeric',
loc_county = 'factor',
age = 'numeric',
flag_fireplace = 'numeric',
num_tax_building = 'numeric',
num_tax_total = 'numeric',
num_tax_land = 'numeric',
num_unit = 'numeric',
quality_factor = 'factor',
heating_factor = 'factor',
prop_living = 'numeric_character',
build_land_prop = 'numeric_character')
data <- house_only16_mv
str(house_only16_mv2)
i = 'prop_living'
house_only16_mv[[i]] <- as.numeric(sub(",", ".",  house_only16_mv[[i]], fixed = TRUE))
house_only16_mv2 <- data.frame(house_only16_mv2)
for (i in colnames(house_only16_mv)) {
if (colClasses[i][[1]] == 'numeric') {
house_only16_mv[[i]] <- as.numeric(house_only16_mv[[i]])
} else if (colClasses[[i]] == 'factor') {
house_only16_mv[[i]] <- as.factor(house_only16_mv[[i]])
} else if (colClasses[[i]] == 'numeric_character'){
house_only16_mv[[i]] <- as.numeric(sub(",", ".", house_only16_mv[[i]], fixed = TRUE))
}
}
str(house_only16_mv)
house_only16_mv <- fread('./Data/house_only16_mv.csv', drop = 'V1')
colClasses = c(id_parcel = 'numeric',
num_bathroom = 'numeric',
num_bedroom = 'numeric',
area_live_finished = 'numeric',
flag_tub_or_spa = 'numeric',
loc_latitude = 'numeric',
loc_longitude = 'numeric',
area_lot = 'numeric',
factor = 'factor',
loc_zip  = 'numeric',
loc_county = 'factor',
age = 'numeric',
flag_fireplace = 'numeric',
num_tax_building = 'numeric',
num_tax_total = 'numeric',
num_tax_land = 'numeric',
num_unit = 'numeric',
quality_factor = 'factor',
heating_factor = 'factor',
prop_living = 'numeric_character',
build_land_prop = 'numeric_character')
str(house_only16_mv2)
# make conversions
for (i in colnames(house_only16_mv)) {
if (colClasses[i][[1]] == 'numeric') {
house_only16_mv[[i]] <- as.numeric(house_only16_mv[[i]])
} else if (colClasses[[i]] == 'factor') {
house_only16_mv[[i]] <- as.factor(house_only16_mv[[i]])
} else if (colClasses[[i]] == 'numeric_character'){
house_only16_mv[[i]] <- as.numeric(sub(",", ".", house_only16_mv[[i]], fixed = TRUE))
}
}
str(house_only16_mv2)
str(house_only16_mv)
house_only16_mv <- fread('./Data/house_only16_mv.csv', drop = 'V1')
colClasses = c(id_parcel = 'numeric',
num_bathroom = 'numeric',
num_bedroom = 'numeric',
area_live_finished = 'numeric',
flag_tub_or_spa = 'numeric',
loc_latitude = 'numeric',
loc_longitude = 'numeric',
area_lot = 'numeric',
factor = 'factor',
loc_zip  = 'numeric',
loc_county = 'factor',
age = 'numeric',
flag_fireplace = 'numeric',
num_tax_building = 'numeric',
num_tax_total = 'numeric',
num_tax_land = 'numeric',
num_unit = 'numeric',
quality_factor = 'factor',
heating_factor = 'factor',
prop_living = 'numeric_character',
build_land_prop = 'numeric_character')
# make conversions
for (i in colnames(house_only16_mv)) {
if (colClasses[i][[1]] == 'numeric') {
house_only16_mv[[i]] <- as.numeric(house_only16_mv[[i]])
} else if (colClasses[[i]] == 'factor') {
house_only16_mv[[i]] <- as.factor(house_only16_mv[[i]])
} else if (colClasses[[i]] == 'numeric_character'){
house_only16_mv[[i]] <- as.numeric(sub(",", ".", house_only16_mv[[i]], fixed = TRUE))
}
}
str(house_only16_mv)
str(house_only16_mv)
#########################################################
### XG Boost -----------------------------------
#########################################################
"note: Xgboost manages only numeric vectors.
For many machine learning algorithms, using correlated features is not a good idea.
It may sometimes make prediction less accurate, and most of the time make interpretation of the model
almost impossible. GLM, for instance, assumes that the features are uncorrelated.
Fortunately, decision tree algorithms (including boosted trees) are very robust to these features.
Therefore we have nothing to do to manage this situation.
"
library(xgboost)
library(Matrix)
library(mlr)
library(parallel)
library(parallelMap)
library(randomForest)
#rm(list=ls())
house_only16_mv <- fread('./Data/house_only16_mv.csv', drop = 'V1')
colClasses = c(id_parcel = 'numeric',
num_bathroom = 'numeric',
num_bedroom = 'numeric',
area_live_finished = 'numeric',
flag_tub_or_spa = 'numeric',
loc_latitude = 'numeric',
loc_longitude = 'numeric',
area_lot = 'numeric',
factor = 'factor',
loc_zip  = 'numeric',
loc_county = 'factor',
age = 'numeric',
flag_fireplace = 'numeric',
num_tax_building = 'numeric',
num_tax_total = 'numeric',
num_tax_land = 'numeric',
num_unit = 'numeric',
quality_factor = 'factor',
heating_factor = 'factor',
prop_living = 'numeric_character',
build_land_prop = 'numeric_character')
# make conversions
for (i in colnames(house_only16_mv)) {
if (colClasses[i][[1]] == 'numeric') {
house_only16_mv[[i]] <- as.numeric(house_only16_mv[[i]])
} else if (colClasses[[i]] == 'factor') {
house_only16_mv[[i]] <- as.factor(house_only16_mv[[i]])
} else if (colClasses[[i]] == 'numeric_character'){
house_only16_mv[[i]] <- as.numeric(sub(",", ".", house_only16_mv[[i]], fixed = TRUE))
}
}
str(house_only16_mv)
rm(list=ls())
house_only16_mv <- fread('./Data/house_only16_mv.csv', drop = 'V1')
colClasses = c(id_parcel = 'numeric',
num_bathroom = 'numeric',
num_bedroom = 'numeric',
area_live_finished = 'numeric',
flag_tub_or_spa = 'numeric',
loc_latitude = 'numeric',
loc_longitude = 'numeric',
area_lot = 'numeric',
factor = 'factor',
loc_zip  = 'numeric',
loc_county = 'factor',
age = 'numeric',
flag_fireplace = 'numeric',
num_tax_building = 'numeric',
num_tax_total = 'numeric',
num_tax_land = 'numeric',
num_unit = 'numeric',
quality_factor = 'factor',
heating_factor = 'factor',
prop_living = 'numeric_character',
build_land_prop = 'numeric_character')
# make conversions
for (i in colnames(house_only16_mv)) {
if (colClasses[i][[1]] == 'numeric') {
house_only16_mv[[i]] <- as.numeric(house_only16_mv[[i]])
} else if (colClasses[[i]] == 'factor') {
house_only16_mv[[i]] <- as.factor(house_only16_mv[[i]])
} else if (colClasses[[i]] == 'numeric_character'){
house_only16_mv[[i]] <- as.numeric(sub(",", ".", house_only16_mv[[i]], fixed = TRUE))
}
}
str(house_only16_mv)
### PART 1: DATA PREPROCESSING ###--------------------------------------
# select the dataframe
data = na.omit(house_only16_mv)
##set the seed to make your partition reproducible
set.seed(123)
smp_size <- floor(0.75 * nrow(data)) ## 75% of the sample size
train_ind <- sample(seq_len(nrow(data)), size = smp_size)
# features we want to omit for the model
omit <- c('id_parcel', 'loc_latitude', 'loc_longitude', 'loc_zip', 'loc_county', 'num_tax_building', 'num_tax_land', 'factor')
# Split the data into train and test
train16 <- data[train_ind,]
test16 <- data[-train_ind, ]
# define training label = dependent variable
output_vector = as.matrix(train16[,'num_tax_total'])
test_vector = as.matrix(test16[,'num_tax_total'])
#omit variables and convert to numeric again
train16 <- train16 %>% select(-omit)
#train16 <- data.frame(sapply(train16, as.numeric))
test16 <- test16 %>% select(-omit)
#test16 <- data.frame(sapply(test16, as.numeric))
# convert categorical factor into dummy variables using one-hot encoding
sparse_matrix_train <- sparse.model.matrix(num_tax_total~.-1, data = train16)
sparse_matrix_test <- sparse.model.matrix(num_tax_total~.-1, data = test16)
# check the dimnames crated by the one-hot encoding
sparse_matrix_train@Dimnames[[2]]
# they should both be of equal length
nrow(sparse_matrix)
nrow(output_vector)
# Create a dense matrix
dtrain <- xgb.DMatrix(data = sparse_matrix_train, label = output_vector)
dtest <- xgb.DMatrix(data = sparse_matrix_test, label=test_vector)
nrow(dtrain)
params <- list(booster = "gbtree",
objective = "reg:squarederror",
eta=0.3, # learning rate, between 0 and 1
gamma=0, # regularization (prevents overfitting), higher means more penality for large coef
max_depth=6, # max depth of trees, the more deep the more complex and overfitting
min_child_weight=1, # min number of instances per child node, blocks potential feature interaction and thus overfitting
subsample=1, # number of observations per tree, typically between 0.5 - 0.8
colsample_bytree=1) # number of variables per tree, typically between 0.5 - 0.9
# using cross-validation to find optimal nrounds parameter
xgbcv <- xgb.cv(params = params,
data = dtrain,
nrounds = 100,
nfold = 5,
showsd = T, # whether to show standard deviation of cv
stratified = T,
print_every_n = 1,
early_stopping_rounds = 20, # stop if we don't see much improvement
maximize = F,
verbose = 2)
# Result of best iteration
xgbcv$best_iteration
# first training with optimized nround
xgb1 <- xgb.train(params = params,
data = dtrain,
nrounds = xgbcv$best_iteration,
watchlist = list(val = dtest, train = dtrain),
early_stopping_rounds = 20,
maximize = F,
eval_metric = "rmse"
)
# model prediction
xgb1_pred <- predict(xgb1, dtest)
rmse <- sqrt(mean((xgb1_pred - test_vector)^2))
print(rmse)
print(head(xgb1_pred))
print(head(test_vector))
# plot the most important leaflets
xgb.plot.multi.trees(feature_names = names(dtrain),
model = xgb1)
# Plot importance
importance <- xgb.importance(feature_names = colnames(sparse_matrix_train), model = xgb1)
xgb.plot.importance(importance_matrix = importance, top_n = 15)
set.seed(0)
# create tasks for learner
traintask <- makeClassifTask(data = train16, target = 'num_tax_total')
testtask <- makeClassifTask(data = test16, target = 'num_tax_total')
# create dummy features, as classif.xgboost does not support factors
traintask <- createDummyFeatures(obj = traintask)
testtask <- createDummyFeatures(obj = traintask)
# create learner
# fix number of rounds and eta
lrn <- makeLearner("classif.xgboost", predict.type = "response")
lrn$par.vals <- list(objective="reg:squarederror",
eval_metric="rmse",
nrounds=100L,
eta=0.1)
# create tasks for learner
traintask <- makeClassifTask(data = train16, target = 'num_tax_total')
str(train16)
fact_col <- colnames(train)[sapply(train16,is.character)]
fact_col <- colnames(train16)[sapply(train16,is.character)]
for(i in fact_col) set(train16,j=i,value = factor(train16[[i]]))
for (i in fact_col) set(test16,j=i,value = factor(test16[[i]]))
# create tasks for learner
traintask <- makeClassifTask(data = train16, target = 'num_tax_total')
View(train16)
str(train16)
# create tasks for learner
traintask <- makeClassifTask(data = train16, target = 'num_tax_total')
# create tasks for learner
traintask <- RegrTask(data = train16, target = 'num_tax_total')
# create tasks for learner
traintask <- RegTask(data = train16, target = 'num_tax_total')
library(mlr)
# create tasks for learner
traintask <- RegrTask(data = train16, target = 'num_tax_total')
# create tasks for learner
traintask <- makeRegrTask(data = train16, target = 'num_tax_total')
testtask <- makeRegrTask(data = test16, target = 'num_tax_total')
# create dummy features, as classif.xgboost does not support factors
traintask <- createDummyFeatures(obj = traintask)
testtask <- createDummyFeatures(obj = traintask)
# create learner
# fix number of rounds and eta
lrn <- makeLearner("classif.xgboost", predict.type = "response")
lrn$par.vals <- list(objective="reg:squarederror",
eval_metric="rmse",
nrounds=100L,
eta=0.1)
# set parameter space
params <- makeParamSet(makeDiscreteParam("booster",
values = c("gbtree","gblinear")),
makeIntegerParam("max_depth",lower = 3L,upper = 10L),
makeNumericParam("min_child_weight",lower = 1L,upper = 10L),
makeNumericParam("subsample",lower = 0.5,upper = 1),
makeNumericParam("colsample_bytree",lower = 0.5,upper = 1))
# set resampling strategy
# as we don't have enough observations for certain classes we cannot do stratification
# e.g. we may not have 5 observations for a house with the factor and class 'wood'
rdesc <- makeResampleDesc("CV",stratify = F, iters=5L)
#search strategy
# instead of a grid search we use a random search strategy to find the best parameters
ctrl <- makeTuneControlRandom(maxit = 10L)
#set parallel backend
parallelStartSocket(cpus = detectCores())
#parameter tuning
mytune <- tuneParams(learner = lrn,
task = traintask,
resampling = rdesc,
measures = acc,
par.set = params,
control = ctrl,
show.info = T)
# create learner
# fix number of rounds and eta
lrn <- makeLearner("regr.xgboost", predict.type = "response")
?makeLearner
# create learner
# fix number of rounds and eta
lrn <- makeLearner("regr.xgboost", predict.type = "response")
lrn$par.vals <- list(objective="reg:squarederror",
eval_metric="rmse",
nrounds=100L,
eta=0.1)
# set parameter space
params <- makeParamSet(makeDiscreteParam("booster",
values = c("gbtree","gblinear")),
makeIntegerParam("max_depth",lower = 3L,upper = 10L),
makeNumericParam("min_child_weight",lower = 1L,upper = 10L),
makeNumericParam("subsample",lower = 0.5,upper = 1),
makeNumericParam("colsample_bytree",lower = 0.5,upper = 1))
# set resampling strategy
# as we don't have enough observations for certain classes we cannot do stratification
# e.g. we may not have 5 observations for a house with the factor and class 'wood'
rdesc <- makeResampleDesc("CV",stratify = F, iters=5L)
#search strategy
# instead of a grid search we use a random search strategy to find the best parameters
ctrl <- makeTuneControlRandom(maxit = 10L)
#set parallel backend
parallelStartSocket(cpus = detectCores())
#parameter tuning
mytune <- tuneParams(learner = lrn,
task = traintask,
resampling = rdesc,
measures = acc,
par.set = params,
control = ctrl,
show.info = T)
#parameter tuning
mytune <- tuneParams(learner = lrn,
task = traintask,
resampling = rdesc,
measures = rmse,
par.set = params,
control = ctrl,
show.info = T)
#parameter tuning
mytune <- tuneParams(learner = lrn,
task = traintask,
resampling = rdesc,
measures = 'rmse',
par.set = params,
control = ctrl,
show.info = T)
#parameter tuning
mytune <- tuneParams(learner = lrn,
task = traintask,
resampling = rdesc,
measures = measureRMSE,
par.set = params,
control = ctrl,
show.info = T)
rm(rmse)
#parameter tuning
mytune <- tuneParams(learner = lrn,
task = traintask,
resampling = rdesc,
measures = list(rmse),
par.set = params,
control = ctrl,
show.info = T)
