maximize = F,
eval_metric = "rmse")
# first training with optimized nround
xgb3 <- xgb.train(data = dtrain,
booster = "gblinear",
objective = "reg:squarederror",
eta=0.3, # learning rate, between 0 and 1
max_depth = mytune$x$max_depth, # max depth of trees, the more deep the more complex and overfitting
min_child_weight = mytune$x$min_child_weight, # min number of instances per child node, blocks potential feature interaction and thus overfitting
subsample= mytune$x$subsample, # number of observations per tree, typically between 0.5 - 0.8
colsample_bytree = mytune$x$colsample_bytree, # number of variables per tree, typically between 0.5 - 0.9
nrounds = xgbcv$best_iteration,
watchlist = list(test = dtest, train = dtrain),
early_stopping_rounds = 20,
maximize = F,
eval_metric = "rmse")
# first training with optimized nround
xgb3 <- xgb.train(data = dtrain,
booster = "gblinear",
objective = "reg:squarederror",
eta=0.3, # learning rate, between 0 and 1
gamma=0, # regularization (prevents overfitting), higher means more penality for large coef
max_depth = mytune$x$max_depth, # max depth of trees, the more deep the more complex and overfitting
min_child_weight = mytune$x$min_child_weight, # min number of instances per child node, blocks potential feature interaction and thus overfitting
subsample= mytune$x$subsample, # number of observations per tree, typically between 0.5 - 0.8
colsample_bytree = mytune$x$colsample_bytree, # number of variables per tree, typically between 0.5 - 0.9
nrounds = xgbcv$best_iteration,
watchlist = list(test = dtest, train = dtrain),
early_stopping_rounds = 20,
maximize = F,
eval_metric = "rmse")
# first training with optimized nround
xgb3 <- xgb.train(data = dtrain,
booster = "gblinear",
objective = "reg:squarederror",
eta=0.3, # learning rate, between 0 and 1
gamma=0, # regularization (prevents overfitting), higher means more penality for large coef
max_depth = mytune$x$max_depth, # max depth of trees, the more deep the more complex and overfitting
min_child_weight = mytune$x$min_child_weight, # min number of instances per child node, blocks potential feature interaction and thus overfitting
subsample= mytune$x$subsample, # number of observations per tree, typically between 0.5 - 0.8
colsample_bytree = mytune$x$colsample_bytree, # number of variables per tree, typically between 0.5 - 0.9
nrounds = xgbcv$best_iteration,
watchlist = list(test = dtest, train = dtrain),
early_stopping_rounds = 20,
maximize = F,
eval_metric = "rmse")
# model prediction
xgb3_pred <- predict(xgb3, dtest)
rmse_xgb3 <- sqrt(mean((xgb3_pred - test_vector)^2))
r2_xgb3 <- 1 - sum((test_vector-xgb3_pred)^2) / sum((test_vector-mean(xg3_pred))^2)
# COMPARE RMSE and R2 -----------------------------
comparison <- data.frame(matrix(data = NA, nrow = 3, ncol = 2, dimnames = list(c('model 1', 'model 2', 'model 3'), c('RMSE', 'R2'))))
comparison$RMSE[1] <- rmse_xgb1
comparison$RMSE[2] <- rmse_xgb2
comparison$RMSE[3] <- rmse_xgb3
comparison$R2[1] <- r2_xgb1
comparison$R2[3] <- r2_xgb2
comparison$R2[3] <- r2_xgb3
# model prediction
xgb3_pred <- predict(xgb3, dtest)
rmse_xgb3 <- sqrt(mean((xgb3_pred - test_vector)^2))
r2_xgb3 <- 1 - sum((test_vector-xgb3_pred)^2) / sum((test_vector-mean(xg3_pred))^2)
r2_xgb3 <- 1 - sum((test_vector-xgb3_pred)^2) / sum((test_vector-mean(xgb3_pred))^2)
r2_xgb2 <- 1 - sum((test_vector-xgb2_pred)^2) / sum((test_vector-mean(xgb2_pred))^2)
comparison$R2[3] <- r2_xgb2
comparison$R2[3] <- r2_xgb3
comparison$R2[2] <- r2_xgb2
comparison$R2[3] <- r2_xgb3
# plot the most important leaflets
xgb.plot.multi.trees(feature_names = names(dtrain),
model = xgb2)
# create dummy features, as classif.xgboost does not support factors
traintask <- createDummyFeatures(obj = traintask)
testtask <- createDummyFeatures(obj = traintask)
# create learner
# fix number of rounds and eta
lrn <- makeLearner("regr.xgboost", predict.type = "response")
lrn$par.vals <- list(objective="reg:squarederror",
eval_metric="rmse",
nrounds=100L,
eta=0.1)
# set parameter space
params <- makeParamSet(makeDiscreteParam("booster",
values = c("gbtree")),
makeIntegerParam("max_depth",lower = 3L,upper = 10L),
makeNumericParam("min_child_weight",lower = 1L,upper = 10L),
makeNumericParam("subsample",lower = 0.5,upper = 1),
makeNumericParam("colsample_bytree",lower = 0.5,upper = 1))
# set resampling strategy
'??'# as we don't have enough observations for certain classes we cannot do stratification
'??'# e.g. we may not have 5 observations for a house with the factor and class 'wood'
# If you have many classes for a classification type predictive modeling problem or the classes are imbalanced
#(there are a lot more instances for one class than another), it can be a good idea to create stratified folds when performing cross validation.
rdesc <- makeResampleDesc("CV",stratify = F, iters=5L)
# search strategy
# instead of a grid search we use a random search strategy to find the best parameters
ctrl <- makeTuneControlRandom(maxit = 10L)
# set parallel backend
parallelStartSocket(cpus = detectCores())
# parameter tuning
mytune <- tuneParams(learner = lrn,
task = traintask,
resampling = rdesc,
measures = list(rmse),
par.set = params,
control = ctrl,
show.info = T)
# print the optimal parameters
mytune
# take the parameters of mytune
params <- list(booster = "gbtree",
objective = "reg:squarederror",
eta=0.3, # learning rate, between 0 and 1
gamma=0, # regularization (prevents overfitting), higher means more penality for large coef
max_depth = mytune$x$max_depth, # max depth of trees, the more deep the more complex and overfitting
min_child_weight = mytune$x$min_child_weight, # min number of instances per child node, blocks potential feature interaction and thus overfitting
subsample= mytune$x$subsample, # number of observations per tree, typically between 0.5 - 0.8
colsample_bytree = mytune$x$colsample_bytree) # number of variables per tree, typically between 0.5 - 0.9
# using cross-validation to find optimal nrounds parameter
xgbcv <- xgb.cv(params = params,
data = dtrain,
nrounds = 100,
nfold = 10,
showsd = T, # whether to show standard deviation of cv
stratified = T,
print_every_n = 1,
early_stopping_rounds = 20, # stop if we don't see much improvement
maximize = F,
verbose = 2)
# Result of best iteration
xgbcv$best_iteration
# first training with optimized nround
xgb2 <- xgb.train(params = params,
data = dtrain,
nrounds = xgbcv$best_iteration,
watchlist = list(test = dtest, train = dtrain),
early_stopping_rounds = 20,
maximize = F,
eval_metric = "rmse"
)
# model prediction
xgb2_pred <- predict(xgb2, dtest)
rmse_xgb2 <- sqrt(mean((xgb2_pred - test_vector)^2))
r2_xgb2 <- 1 - sum((test_vector-xgb2_pred)^2) / sum((test_vector-mean(xgb2_pred))^2)
# MODEL 3 - linear boosting ---------------------------
"Note that linear boosting is great to capture linear relationships while trees are better at
capturing non-linear relationship"
# take the parameters of mytune
params <- list(booster = "gblinear",
objective = "reg:squarederror",
eta=0.3, # learning rate, between 0 and 1
gamma=0, # regularization (prevents overfitting), higher means more penality for large coef
max_depth = mytune$x$max_depth, # max depth of trees, the more deep the more complex and overfitting
min_child_weight = mytune$x$min_child_weight, # min number of instances per child node, blocks potential feature interaction and thus overfitting
subsample= mytune$x$subsample, # number of observations per tree, typically between 0.5 - 0.8
colsample_bytree = mytune$x$colsample_bytree) # number of variables per tree, typically between 0.5 - 0.9
# using cross-validation to find optimal nrounds parameter
xgbcv <- xgb.cv(params = params,
data = dtrain,
nrounds = 100,
nfold = 10,
showsd = T, # whether to show standard deviation of cv
stratified = T,
print_every_n = 1,
early_stopping_rounds = 20, # stop if we don't see much improvement
maximize = F,
verbose = 2)
# Result of best iteration
xgbcv$best_iteration
# first training with optimized nround
xgb3 <- xgb.train(data = dtrain,
booster = "gblinear",
objective = "reg:squarederror",
eta=0.3, # learning rate, between 0 and 1
gamma=0, # regularization (prevents overfitting), higher means more penality for large coef
max_depth = mytune$x$max_depth, # max depth of trees, the more deep the more complex and overfitting
min_child_weight = mytune$x$min_child_weight, # min number of instances per child node, blocks potential feature interaction and thus overfitting
subsample= mytune$x$subsample, # number of observations per tree, typically between 0.5 - 0.8
colsample_bytree = mytune$x$colsample_bytree, # number of variables per tree, typically between 0.5 - 0.9
nrounds = xgbcv$best_iteration,
watchlist = list(test = dtest, train = dtrain),
early_stopping_rounds = 20,
maximize = F,
eval_metric = "rmse")
# model prediction
xgb3_pred <- predict(xgb3, dtest)
rmse_xgb3 <- sqrt(mean((xgb3_pred - test_vector)^2))
r2_xgb3 <- 1 - sum((test_vector-xgb3_pred)^2) / sum((test_vector-mean(xgb3_pred))^2)
# COMPARE RMSE and R2 -----------------------------
comparison <- data.frame(matrix(data = NA, nrow = 3, ncol = 2, dimnames = list(c('model 1', 'model 2', 'model 3'), c('RMSE', 'R2'))))
comparison$RMSE[1] <- rmse_xgb1
comparison$RMSE[2] <- rmse_xgb2
comparison$RMSE[3] <- rmse_xgb3
comparison$R2[1] <- r2_xgb1
comparison$R2[2] <- r2_xgb2
comparison$R2[3] <- r2_xgb3
comparison <- data.frame(matrix(data = NA, nrow = 3, ncol = 2, dimnames = list(c('xgb_tree 1', 'xgb_tree 2', 'xgb_linear 3'), c('RMSE', 'R2'))))
comparison$RMSE[1] <- rmse_xgb1
comparison$RMSE[2] <- rmse_xgb2
comparison$RMSE[3] <- rmse_xgb3
comparison$R2[1] <- r2_xgb1
comparison$R2[2] <- r2_xgb2
comparison$R2[3] <- r2_xgb3
comparison
# plot the most important leaflets
xgb.plot.multi.trees(feature_names = names(dtrain),
model = xgb2)
# Plot importance
importance2 <- xgb.importance(feature_names = colnames(sparse_matrix_train), model = xgb2)
xgb.plot.importance(importance_matrix = importance2, top_n = 15)
class(test_vector)
# Plot predicted vs. actual
ggplot(test_vector)
library(tidyverse)
# Plot predicted vs. actual
ggplot(test_vector)
# Plot predicted vs. actual
actual = data.frame(test_vector)
ggplot(actual)
View(actual)
ggplot(actual, aes(x = , y= num_tax_total)
, aes(x = num_bedroom, y = log(num_tax_building))) +
geom_point()
ggplot(actual, aes(x = row.names(actual) , y= num_tax_total)
, aes(x = num_bedroom, y = log(num_tax_building))) +
geom_point()
predicted = data.frame(xgb2_pred)
View(predicted)
ggplot(actual, aes(x = row.names(actual) , y= num_tax_total)) +
geom_point(predicted, aes(x = row.names(actual) , y= num_tax_total))
ggplot(actual, aes(x = row.names(actual) , y= num_tax_total)) +
geom_point(predicted, aes(x = row.names(actual) , y= xgb2_pred))
ggplot(actual, aes(x = row.names(actual) , y= num_tax_total))
# Plot predicted vs. actual
actual = data.frame(test_vector)
predicted = data.frame(xgb2_pred)
ggplot(data = actual, aes(x = row.names(actual) , y= num_tax_total)) +
geom_point(data = predicted, aes(x = row.names(actual) , y= xgb2_pred))
merged_df <- merge(actual, predicted)
View(merged_df)
merged_df[order(actual),]
merged_df <- merged_df[order(actual),]
ggplot(data = merged_df) +
geom_point(aes(x = row.names(merged_df), y = num_tax_total)) +
geom_point(aes(x = row.names(merged_df), y = xgb2_pred))
ggplot(data = merged_df) +
geom_point(aes(x = row.names(merged_df), y = num_tax_total), color = 'red') +
geom_point(aes(x = row.names(merged_df), y = xgb2_pred), color = 'blue')
View(merged_df)
merged_df$initialindex <- row.names(merged_df)
row.names(merged_df) <- NULL
ggplot(data = merged_df) +
geom_point(aes(x = row.names(merged_df), y = num_tax_total), color = 'red') +
geom_point(aes(x = row.names(merged_df), y = xgb2_pred), color = 'blue')
View(merged_df)
ggplot(data = merged_df) +
geom_point(aes(x = as.numeric(row.names(merged_df)), y = num_tax_total), color = 'red') +
geom_point(aes(x = as.numeric(row.names(merged_df)), y = xgb2_pred), color = 'blue')
xgb2_pred
# take the parameters of mytune
params <- list(booster = "gbtree",
objective = "reg:squarederror",
eta=0.3, # learning rate, between 0 and 1
gamma=0, # regularization (prevents overfitting), higher means more penality for large coef
max_depth = mytune$x$max_depth, # max depth of trees, the more deep the more complex and overfitting
min_child_weight = mytune$x$min_child_weight, # min number of instances per child node, blocks potential feature interaction and thus overfitting
subsample= mytune$x$subsample, # number of observations per tree, typically between 0.5 - 0.8
colsample_bytree = mytune$x$colsample_bytree) # number of variables per tree, typically between 0.5 - 0.9
# using cross-validation to find optimal nrounds parameter
xgbcv <- xgb.cv(params = params,
data = dtrain,
nrounds = 100,
nfold = 10,
showsd = T, # whether to show standard deviation of cv
stratified = T,
print_every_n = 1,
early_stopping_rounds = 20, # stop if we don't see much improvement
maximize = F,
verbose = 2)
# Result of best iteration
xgbcv$best_iteration
# first training with optimized nround
xgb2 <- xgb.train(params = params,
data = dtrain,
nrounds = xgbcv$best_iteration,
watchlist = list(test = dtest, train = dtrain),
early_stopping_rounds = 20,
maximize = F,
eval_metric = "rmse"
)
# model prediction
xgb2_pred <- predict(xgb2, dtest)
rmse_xgb2 <- sqrt(mean((xgb2_pred - test_vector)^2))
r2_xgb2 <- 1 - sum((test_vector-xgb2_pred)^2) / sum((test_vector-mean(xgb2_pred))^2)
xgb2_pred
test_vector
# Plot predicted vs. actual
actual = data.frame(test_vector)
predicted = data.frame(xgb2_pred)
merged_df <- merge(actual, predicted)
merged_df <- merged_df[order(actual),]
merged_df$initialindex <- row.names(merged_df)
row.names(merged_df) <- NULL
max(predicted)
min(predicted)
ggplot(data = merged_df) +
geom_line(aes(x = as.numeric(row.names(merged_df)), y = num_tax_total), color = 'red') +
geom_line(aes(x = as.numeric(row.names(merged_df)), y = xgb2_pred), color = 'blue')
max(actual)
min(actual)
colnames(merged_df) <- c('Log Total Value', 'Predicted', 'Initial Index')
colnames(merged_df) <- c('Log_Total_Value', 'Predicted', 'Initial_ Index')
ggplot(data = merged_df) +
geom_point(aes(x = as.numeric(row.names(merged_df)), y = Log_Total_Value), color = 'red') +
geom_point(aes(x = as.numeric(row.names(merged_df)), y = Predicted), color = 'blue')
# Plot predicted vs. actual
actual = data.frame(test_vector)
predicted = data.frame(xgb2_pred)
merged_df <- merge(actual, predicted)
merged_df <- merged_df[order(actual),]
merged_df$initialindex <- row.names(merged_df)
row.names(merged_df) <- NULL
max(predicted)
min(predicted)
max(actual)
min(actual)
hist(predicted)
hist(predicted$xgb2_pred)
hist(actual$num_tax_total)
ggplot(data = merged_df) +
geom_point(aes(x = as.numeric(row.names(merged_df)), y = xgb2_pred), color = 'blue')
View(predicted)
View(predicted)
View(merged_df)
ggplot(data = merged_df) +
geom_point(aes(x = as.numeric(row.names(merged_df)), y = merged_df$xgb2_pred), color = 'blue')
ggplot(data = merged_df) +
geom_point(aes(x = as.numeric(row.names(merged_df)), y = xgb2_pred), color = 'blue')
?merge
merged_df <- merge(actual, predicted, by = row.names)
merged_df <- merge(actual, predicted, by = 0)
# Plot predicted vs. actual
actual = data.frame(test_vector)
predicted = data.frame(xgb2_pred)
merged_df <- merge(actual, predicted, by = 0) #by 0 merges based on index
View(merged_df)
remove(merged_df)
# Plot predicted vs. actual
actual = data.frame(test_vector)
predicted = data.frame(xgb2_pred)
merged_df <- merge(actual, predicted, by = 0) #by 0 merges based on index
View(merged_df)
View(actual)
View(comparison)
View(predicted)
merged_df <- merge(actual, predicted, by.x = 0) #by 0 merges based on index
?join
inner_join(actual, predicted)
merged_df <- cbind(test_vector, xgb2_pred) #by 0 merges based on index
View(merged_df)
# Plot predicted vs. actual
merged_df <- data.frame(cbind(test_vector, xgb2_pred)) #by 0 merges based on index
merged_df <- merged_df[order(actual),]
merged_df$initialindex <- row.names(merged_df)
row.names(merged_df) <- NULL
hist(predicted$xgb2_pred)
hist(actual$num_tax_total)
ggplot(data = merged_df) +
geom_point(aes(x = as.numeric(row.names(merged_df)), y = num_tax_total), color = 'red') +
geom_point(aes(x = as.numeric(row.names(merged_df)), y = xgb2_pred), color = 'blue')
ggplot(data = merged_df) +
geom_point(aes(x = as.numeric(row.names(merged_df)), y = num_tax_total), color = 'red') +
geom_point(aes(x = as.numeric(row.names(merged_df)), y = xgb2_pred), color = 'blue') +
labs()
mdf <- reshape2::melt(merged_df, id.var = "x")
mdf <- reshape2::melt(merged_df, id.var = "initialindex")
View(mdf)
remove(mdf)
# from wide to long
merged_df <- reshape2::melt(merged_df, id.var = "initialindex")
# Plot predicted vs. actual
ggplot(data = merged_df, aes(x = as.numeric(row.names(merged_df)), y = variable)) +
geom_point()
# Plot predicted vs. actual
ggplot(data = merged_df, aes(x = initialindex, y = variable)) +
geom_point()
# Plot predicted vs. actual
ggplot(data = merged_df) +
geom_point(aes(x = as.numeric(row.names(merged_df)), y = num_tax_total), color = 'red') +
geom_point(aes(x = as.numeric(row.names(merged_df)), y = xgb2_pred), color = 'blue') +
l
# Plot predicted vs. actual
ggplot(data = merged_df) +
geom_point(aes(x = as.numeric(row.names(merged_df)), y = num_tax_total), color = 'red') +
geom_point(aes(x = as.numeric(row.names(merged_df)), y = xgb2_pred), color = 'blue')
# merge dataframes and make it from wide to long
merged_df <- data.frame(cbind(test_vector, xgb2_pred)) #by 0 merges based on index
merged_df <- merged_df[order(actual),]
merged_df$initialindex <- row.names(merged_df)
row.names(merged_df) <- NULL
# Plot predicted vs. actual
ggplot(data = merged_df) +
geom_point(aes(x = as.numeric(row.names(merged_df)), y = num_tax_total), color = 'red') +
geom_point(aes(x = as.numeric(row.names(merged_df)), y = xgb2_pred), color = 'blue')
ggplot(data = merged_df) +
geom_point(aes(x = as.numeric(row.names(merged_df)), y = num_tax_total), color = 'red') +
geom_point(aes(x = as.numeric(row.names(merged_df)), y = xgb2_pred), color = 'blue') +
ggtitle('Actual vs. predicted values') +
scale_color_manual(values = colors)
# Plot predicted vs. actual
ggplot(data = merged_df) +
geom_point(aes(x = as.numeric(row.names(merged_df)), y = num_tax_total), color = 'predicted') +
geom_point(aes(x = as.numeric(row.names(merged_df)), y = xgb2_pred), color = 'actual') +
ggtitle('Actual vs. predicted values') +
scale_color_manual(values = colors)
# Plot predicted vs. actual
ggplot(data = merged_df, aes(x = as.numeric(row.names(merged_df)))) +
geom_point(aes(y = num_tax_total, color = 'predicted')) +
geom_point(aes(y = xgb2_pred, color = 'actual')) +
ggtitle('Actual vs. predicted values') +
scale_color_manual(values = colors)
# Plot predicted vs. actual
ggplot(data = merged_df, aes(x = as.numeric(row.names(merged_df)))) +
geom_point(aes(y = num_tax_total, color = 'predicted')) +
geom_point(aes(y = xgb2_pred, color = 'actual')) +
ggtitle('Actual vs. predicted values') +
scale_color_manual(values = c('predicted', 'actual'))
# Plot predicted vs. actual
ggplot(data = merged_df, aes(x = as.numeric(row.names(merged_df)))) +
geom_point(aes(y = num_tax_total, color = 'predicted')) +
geom_point(aes(y = xgb2_pred, color = 'actual')) +
ggtitle('Actual vs. predicted values') +
scale_color_manual(values = c('blue', 'red'))
# Plot predicted vs. actual
ggplot(data = merged_df, aes(x = as.numeric(row.names(merged_df)))) +
geom_point(aes(y = xgb2_pred, color = 'actual')) +
geom_point(aes(y = num_tax_total, color = 'predicted')) +
ggtitle('Actual vs. predicted values') +
scale_color_manual(values = c('blue', 'red'))
# Plot predicted vs. actual
colors <- c("actual" = "blue", "predicted" = "red")
ggplot(data = merged_df, aes(x = as.numeric(row.names(merged_df)))) +
geom_point(aes(y = xgb2_pred, color = 'actual')) +
geom_point(aes(y = num_tax_total, color = 'predicted')) +
ggtitle('Actual vs. predicted values') +
scale_color_manual(values = colors)
ggplot(data = merged_df, aes(x = as.numeric(row.names(merged_df)))) +
geom_point(aes(y = xgb2_pred, color = 'predicted')) +
geom_point(aes(y = num_tax_total, color = 'actual')) +
ggtitle('Actual vs. predicted values') +
scale_color_manual(values = colors)
# Plot predicted vs. actual
Values <- c("actual" = "red", "predicted" = "blue")
ggplot(data = merged_df, aes(x = as.numeric(row.names(merged_df)))) +
geom_point(aes(y = xgb2_pred, color = 'predicted')) +
geom_point(aes(y = num_tax_total, color = 'actual')) +
ggtitle('Actual vs. predicted values') +
scale_color_manual(values = Values)
# Plot predicted vs. actual
colors <- c("actual" = "red", "predicted" = "blue")
ggplot(data = merged_df, aes(x = as.numeric(row.names(merged_df)))) +
geom_point(aes(y = xgb2_pred, color = 'predicted')) +
geom_point(aes(y = num_tax_total, color = 'actual')) +
ggtitle('Actual vs. predicted values') +
scale_color_manual(values = colors)
plot <- ggplot(data = merged_df, aes(x = as.numeric(row.names(merged_df)))) +
geom_point(aes(y = xgb2_pred, color = 'predicted')) +
geom_point(aes(y = num_tax_total, color = 'actual')) +
ggtitle('Actual vs. predicted values') +
scale_color_manual(values = colors)
plot <- ggplot(data = merged_df, aes(x = as.numeric(row.names(merged_df)))) +
geom_point(aes(y = xgb2_pred, color = 'predicted')) +
geom_point(aes(y = num_tax_total, color = 'actual')) +
ggtitle('Actual vs. predicted values') +
scale_color_manual(values = colors)
# SAVE MODELS AND PLOTS -----------------------------
# save plot
ggsae('plot_xgb', plot = plot, device = png())
plot
# save plot
ggsave('plot_xgb', plot = plot, device = png())
# save plot
ggsave('plot_xgb', plot = plot, device = 'png')
long <- reshape2::melt(merged_df, id.var = "initialindex")
View(long)
ggplot(long, aes(x = initialindex, y = value, colour = variable)) +
geom_point()
ggplot(long, aes(x = initialindex, y = value, colour = variable)) +
geom_point()
?gather
df <- gather(merged_df, key = measure, value = Rate,
c("deathpercentage", "tamponadepercentage", "protaminepercentage"))
df <- gather(merged_df, key = measure, value = Rate,
c("xgb2_pred", "num_tax_total"))
View(df)
df <- gather(merged_df, key = variable, value = value,
c("xgb2_pred", "num_tax_total"))
merged_df_long <- gather(merged_df, key = variable, value = value,
c("xgb2_pred", "num_tax_total"))
ggplot(merged_df_long, aes(x=initialindex, y = value, group = variable, colour = variable)) +
geom_line()
View(merged_df_long)
ggplot(merged_df_long, aes(x=initialindex, y = value, group = variable, colour = variable)) +
geom_point()
View(merged_df_long)
plot <- ggplot(data = merged_df, aes(x = as.numeric(row.names(merged_df)))) +
geom_point(aes(y = xgb2_pred, color = 'predicted')) +
geom_point(aes(y = num_tax_total, color = 'actual')) +
ggtitle('Actual vs. predicted values') +
scale_color_manual(values = colors)
plot
plot
