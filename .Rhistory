result_keyness1 <- textstat_keyness(buffett_small_dfm,
tbuffettget = "FINMA")
# Keyness Plot
textplot_keyness(result_keyness1)
}
get_keyness_plot(buffett_corpus, "1977", "1978", TRUE)
get_keyness_plot(buffett_corpus, "", "", FALSE)
get_keyness_plot(buffett_corpus, "2006", "2008", TRUE)
get_topn <- function(chosen_dict, n, type){
# perform dfm analysis based on chosen dictionary
buffett_corpus_lexi <- buffett_corpus %>%
filter(year %in% years) %>%
dfm(.,
groups = "year",
remove = stopwords("english"),
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
dictionary = chosen_dict)
# data cleaning and transformations
df_lex <- convert(buffett_corpus_lexi, to = "data.frame")
df_lex$doc_id <- as.numeric(df_lex$doc_id)
rownames(df_lex) <- df_lex$doc_id
df_lex <- df_lex[,-1]
col_sums <- data.frame(colSums(df_lex)) # count columns
colnames(col_sums) <- "counts" # change colnames
col_sums <- arrange(col_sums, counts) # sort it ascending
topn_counts <- tail(col_sums, n) # get the top n values
topn_counts$topics <- rownames(topn_counts)
# select the top n topics
df_lex <- df_lex %>%
select(topn_counts$topics)
if(type == "percentage") {
# calculate percentages and get back the year
df_lex <- df_lex/rowSums(df_lex)*100
}
# from short to long dataframe for plotting
df_lex$year <- as.numeric(rownames(df_lex))
df_lex <- df_lex %>%
gather(key = "topic", value = count, -year)
# plot the Top n Topics
if(type == "percentage") {
ggplot(df_lex, aes(x = year, y = count, fill = topic)) +
geom_area() +
labs(title= paste0("Change in Top " , n, " topics over the years | ", deparse(substitute(chosen_dict)))) +
ylab('percentage')
} else {
ggplot(df_lex, aes(x = year, y = count)) +
geom_line(aes(color = topic, linetype = topic)) +
labs(title= paste0("Change in Top " , n, " topics over the years | ", deparse(substitute(chosen_dict)))) +
ylab('count')}
}
# Define the dictionaries for topics
dict_lexi <- dictionary(file=("./L6/policy_agendas_english.lcd"))
dict_lm_without <- data_dictionary_LoughranMcDonald[-c(1:2,8:9)] # without negative and positive and modal words
dict_nrc_without <- data_dictionary_NRC[-c(6:7)] # without negative and positive
# Define the dictionaries for negative and postive
dict_lm <- data_dictionary_LoughranMcDonald[c(1:2)]
dict_afinn <- data_dictionary_AFINN
dict_nrc <- data_dictionary_NRC[c(6:7)]
# Run the plots for the various dictionaries
get_topn(dict_lexi, 5, "absolute")
get_topn(dict_lm_without, 5, "absolute")
get_topn(dict_nrc_without, 5, "absolute" )
get_topn(dict_lm, 2, "absolute")
get_topn(dict_afinn, 2, "absolute")
get_topn(dict_nrc, 2, "absolute")
get_topn(dict_lexi, 5, "percentage")
get_topn(dict_lm_without, 5, "percentage")
get_topn(dict_nrc_without, 5, "percentage" )
get_topn(dict_lm, 2, "percentage")
get_topn(dict_afinn, 2, "percentage")
get_topn(dict_nrc, 2, "percentage")
lexicon::hash_valence_shifters
# Polarity with valence shifters
sentiment(text.var = buffett_corpus,
polarity_dt = lexicon::hash_sentiment_jockers_rinker,
n.before = 5,
n.after = 2)
library(magrittr)
presidential_debates_2012
presidential_debates_2012 %>%
dplyr::mutate(dialogue_split = get_sentences(dialogue)) %$%
sentiment_by(dialogue_split, list(person, time))
# Transform to dataframe
buffet_df <- data.frame(text = sapply(buffett_corpus, as.character), stringsAsFactors = FALSE)
buffet_df$year <- seq(1977, 2020)
# Make it tidy and join senting with BING lexicon
tidy_letters <- buffet_df %>%
tidytext::unnest_tokens(word, text) %>%   # split text into words
dplyr::anti_join(stop_words, by = "word") %>% # remove stop words
filter(!grepl('[0-9]', word)) %>% # filter out numbers
left_join(get_sentiments("bing"), by = "word") %>%  # add sentiment scores to words
mutate(sentiment = ifelse(is.na(sentiment), "neutral", sentiment)) %>%
group_by(year) %>%
ungroup()
# Calculate sentiment score by letter
letters_sentiment <- tidy_letters %>%
count(year, sentiment) %>%
spread(key = sentiment, value = n) %>%
mutate(sentiment_pct = (positive - negative) / (positive + negative + neutral)) %>%
select(year, sentiment_pct)
ggplot(letters_sentiment, aes(x = year, y = sentiment_pct)) +
geom_bar(aes(fill = sentiment_pct < 0), stat = 'identity') +
geom_text(aes(label = year, hjust = ifelse(sentiment_pct >= 0, -0.15, 1.15)), vjust = 0.5) +
scale_fill_manual(guide = F, values = c('#565b63', '#c40909')) +
coord_flip() +
labs(y='Net Sentiment Ratio',
title='Text sentiment of shareholder letters with BING lexicon')
#########################################################
### XG Boost -----------------------------------
#########################################################
"note: Xgboost manages only numeric vectors.
For many machine learning algorithms, using correlated features is not a good idea.
It may sometimes make prediction less accurate, and most of the time make interpretation of the model
almost impossible. GLM, for instance, assumes that the features are uncorrelated.
Fortunately, decision tree algorithms (including boosted trees) are very robust to these features.
Therefore we have nothing to do to manage this situation.
"
library(xgboost)
library(Matrix)
library(mlr)
library(parallel)
library(parallelMap)
library(randomForest)
library(data.table)
library(dplyr)
library(tidyverse)
setwd('/Users/tgraf/Google Drive/Uni SG/Master/Research Seminar /Repository')
rm(list=ls())
house_only16_mv <- fread('./Data/house_only16_mv.csv', drop = 'V1')
colClasses = c(id_parcel = 'numeric',
num_bathroom = 'numeric',
num_bedroom = 'numeric',
area_live_finished = 'numeric',
flag_tub_or_spa = 'numeric',
loc_latitude = 'numeric',
loc_longitude = 'numeric',
area_lot = 'numeric',
factor = 'factor',
loc_zip  = 'numeric',
loc_county = 'factor',
age = 'numeric',
flag_fireplace = 'numeric',
num_tax_building = 'numeric',
num_tax_total = 'numeric',
num_tax_land = 'numeric',
num_unit = 'numeric',
quality_factor = 'factor',
heating_factor = 'factor',
prop_living = 'numeric_character',
build_land_prop = 'numeric_character')
# make conversions
for (i in colnames(house_only16_mv)) {
if (colClasses[i][[1]] == 'numeric') {
house_only16_mv[[i]] <- as.numeric(house_only16_mv[[i]])
} else if (colClasses[[i]] == 'factor') {
house_only16_mv[[i]] <- as.factor(house_only16_mv[[i]])
} else if (colClasses[[i]] == 'numeric_character'){
house_only16_mv[[i]] <- as.numeric(sub(",", ".", house_only16_mv[[i]], fixed = TRUE))
}
}
str(house_only16_mv)
# select the dataframe
data = na.omit(house_only16_mv)
# use only first 10'000
data = data[1:10000,]
##set the seed to make your partition reproducible
set.seed(123)
smp_size <- floor(0.75 * nrow(data)) ## 75% of the sample size
train_ind <- sample(seq_len(nrow(data)), size = smp_size)
# features we want to omit for the model
omit <- c('id_parcel', 'loc_latitude', 'loc_longitude', 'loc_zip', 'loc_county', 'num_tax_building', 'num_tax_land', 'factor', 'build_land_prop')
# Split the data into train and test
train16 <- data[train_ind,]
test16 <- data[-train_ind, ]
# define training label = dependent variable
output_vector = as.matrix(log(train16[,'num_tax_total']))
test_vector = as.matrix(log(test16[,'num_tax_total']))
#omit variables and convert to numeric again
train16 <- train16 %>% select(-omit)
#train16 <- data.frame(sapply(train16, as.numeric))
test16 <- test16 %>% select(-omit)
#test16 <- data.frame(sapply(test16, as.numeric))
# convert categorical factor into dummy variables using one-hot encoding
sparse_matrix_train <- sparse.model.matrix(log(num_tax_total)~.-1, data = train16)
sparse_matrix_test <- sparse.model.matrix(log(num_tax_total)~.-1, data = test16)
# check the dimnames crated by the one-hot encoding
sparse_matrix_train@Dimnames[[2]]
# Create a dense matrix
dtrain <- xgb.DMatrix(data = sparse_matrix_train, label = output_vector)
dtest <- xgb.DMatrix(data = sparse_matrix_test, label=test_vector)
### PART 2: XGBOOST Training ###  -----------------------------
# # Model 1: Default parameters  -----------------------------
# #Let's start with a standard model and parameters and start optimizing the parameters later from here
#
# params <- list(booster = "gbtree",
#                objective = "reg:squarederror",
#                eta=0.3, # learning rate, between 0 and 1
#                gamma=0, # regularization (prevents overfitting), higher means more penality for large coef
#                max_depth=6, # max depth of trees, the more deep the more complex and overfitting
#                min_child_weight=1, # min number of instances per child node, blocks potential feature interaction and thus overfitting
#                subsample=1, # number of observations per tree, typically between 0.5 - 0.8
#                colsample_bytree=1) # number of variables per tree, typically between 0.5 - 0.9
#
# # using cross-validation to find optimal nrounds parameter
# xgbcv <- xgb.cv(params = params,
#                 data = dtrain,
#                 nrounds = 100,
#                 nfold = 10,
#                 showsd = T, # whether to show standard deviation of cv
#                 stratified = F,
#                 print_every_n = 1,
#                 early_stopping_rounds = 20, # stop if we don't see much improvement
#                 maximize = F,
#                 verbose = 2)
#
# # Result of best iteration
# xgbcv$best_iteration
#
# # first training with optimized nround
# xgb1 <- xgb.train(params = params,
#                   data = dtrain,
#                   nrounds = xgbcv$best_iteration,
#                   watchlist = list(test = dtest, train = dtrain),
#                   early_stopping_rounds = 20,
#                   maximize = F,
#                   eval_metric = "rmse"
# )
#
# # model prediction
# xgb1_pred <- predict(xgb1, dtest)
# rmse_xgb1 <- sqrt(mean((xgb1_pred - test_vector)^2))
# r2_xgb1 <- 1 - sum((test_vector-xgb1_pred)^2) / sum((test_vector-mean(xgb1_pred))^2)
# Find Optimized parameters  -----------------------------
set.seed(123)
fact_col <- colnames(train16)[sapply(train16,is.character)]
for(i in fact_col) set(train16,j=i,value = factor(train16[[i]]))
for (i in fact_col) set(test16,j=i,value = factor(test16[[i]]))
# create tasks for learner
traintask <- makeRegrTask(data = train16, target = 'num_tax_total')
testtask <- makeRegrTask(data = test16, target = 'num_tax_total')
# create dummy features, as classif.xgboost does not support factors
traintask <- createDummyFeatures(obj = traintask)
testtask <- createDummyFeatures(obj = traintask)
# create learner
# fix number of rounds and eta
lrn <- makeLearner("regr.xgboost", predict.type = "response")
lrn$par.vals <- list(objective="reg:squarederror",
eval_metric="rmse",
nrounds=100L,
eta = 0.1)
# set parameter space
params <- makeParamSet(makeDiscreteParam("booster", values = c("gbtree", "dart")),
makeIntegerParam("max_depth",lower = 3L,upper = 10L))
#makeNumericParam("min_child_weight",lower = 1L,upper = 10L),
#makeNumericParam("subsample",lower = 0.2,upper = 1),
# makeNumericParam("colsample_bytree",lower = 0.1,upper = 1),
# makeDiscreteParam("eta", values = c(0.05,0.1,0.2)))
# set resampling strategy
# If you have many classes for a classification type predictive modeling problem or the classes are imbalanced
#(there are a lot more instances for one class than another), it can be a good idea to create stratified folds when performing cross validation.
rdesc <- makeResampleDesc("CV",stratify = F, iters=5L)
# search strategy
# instead of a grid search we use a random search strategy to find the best parameters
ctrl <- makeTuneControlRandom(maxit = 10L)
# set parallel backend
parallelStartSocket(cpus = detectCores())
# parameter tuning
mytune <- tuneParams(learner = lrn,
task = traintask,
resampling = rdesc,
par.set = params,
control = ctrl,
show.info = T)
# print the optimal parameters
mytune
# Model 2: XGBoost with optimized parameters  -----------------------------
# take the parameters of mytune
params <- list(booster = mytune$x$booster,
objective = "reg:squarederror",
eta=0.1, # learning rate, between 0 and 1
gamma=0, # regularization (prevents overfitting), higher means more penality for large coef
max_depth = mytune$x$max_depth) # max depth of trees, the more deep the more complex and overfitting
#min_child_weight = mytune$x$min_child_weight, # min number of instances per child node, blocks potential feature interaction and thus overfitting
#subsample= mytune$x$subsample, # number of observations per tree, typically between 0.5 - 0.8
#colsample_bytree = mytune$x$colsample_bytree) # number of variables per tree, typically between 0.5 - 0.9
# using cross-validation to find optimal nrounds parameter
xgbcv <- xgb.cv(params = params,
data = dtrain,
nrounds = 100L,
nfold = 5,
showsd = T, # whether to show standard deviation of cv
stratified = F,
print_every_n = 1,
early_stopping_rounds = 50, # stop if we don't see much improvement
maximize = F,
verbose = 2)
# Result of best iteration
xgbcv$best_iteration
# first training with optimized nround
xgb2 <- xgb.train(params = params,
data = dtrain,
nrounds = xgbcv$best_iteration,
watchlist = list(test = dtest, train = dtrain),
maximize = F,
eval_metric = "rmse"
)
# model prediction
xgb2_pred <- predict(xgb2, dtest)
rmse_xgb2 <- sqrt(mean((xgb2_pred - test_vector)^2))
r2_xgb2 <- 1 - ( sum((test_vector-xgb2_pred)^2) / sum((test_vector-mean(xgb2_pred))^2) )
str(test_vector)
str(xgb2_pred)
# simple regression
model_lm = lm(num_tax_total ~ ., data = data.frame(dtrain))
# COMPARISON simple regression ----------------------------------
train16_sparse <- data.frame(model.matrix(~ . -1, train16))
test16_sparse <- data.frame(model.matrix(~ . -1, test16))
model_lm = lm(num_tax_total ~ ., data = data.frame(dtrain))
model_lm = lm(num_tax_total ~ ., data = data.frame(train16_sparse))
pred_lm <- predict(model_lm, (test16_sparse))
model_lm = lm(num_tax_total ~ ., data = data.frame(train16_sparse))
pred_lm <- predict(model_lm, (test16_sparse))
# Predict using simple regression
rmse_lm <- sqrt(mean((pred_lm - test_vector)^2))
r2_lm <- 1 - (sum((test_vector-pred_lm)^2) / sum((test_vector-mean(pred_lm))^2) )
str(pred_lm)
str(test_vector)
pred_lm <- log(predict(model_lm, (test16_sparse)))
# Predict using simple regression
rmse_lm <- sqrt(mean((pred_lm - test_vector)^2))
r2_lm <- 1 - (sum((test_vector-pred_lm)^2) / sum((test_vector-mean(pred_lm))^2) )
r2_xgb2 <- 1 - ( sum((test_vector-xgb2_pred)^2) / sum((test_vector-mean(test_vector))^2) )
str(xgb2_pred)
# COMPARISON simple regression ----------------------------------
train16_sparse <- data.frame(model.matrix(~ . -1, train16))
test16_sparse <- data.frame(model.matrix(~ . -1, test16))
model_lm = lm(num_tax_total ~ ., data = data.frame(train16_sparse))
pred_lm <- log(predict(model_lm, (test16_sparse)))
# Predict using simple regression
rmse_lm <- sqrt(mean((pred_lm - test_vector)^2))
r2_lm <- 1 - (sum((test_vector-pred_lm)^2) / sum((test_vector-mean(test_vector))^2) )
#########################################################
### XG Boost -----------------------------------
#########################################################
"note: Xgboost manages only numeric vectors.
For many machine learning algorithms, using correlated features is not a good idea.
It may sometimes make prediction less accurate, and most of the time make interpretation of the model
almost impossible. GLM, for instance, assumes that the features are uncorrelated.
Fortunately, decision tree algorithms (including boosted trees) are very robust to these features.
Therefore we have nothing to do to manage this situation.
"
library(xgboost)
library(Matrix)
library(mlr)
library(parallel)
library(parallelMap)
library(randomForest)
library(data.table)
library(dplyr)
library(tidyverse)
library(tictoc)
tic()
setwd('/Users/tgraf/Google Drive/Uni SG/Master/Research Seminar /Repository')
rm(list=ls())
house_only16_mv <- fread('./Data/house_only16_mv.csv', drop = 'V1')
colClasses = c(id_parcel = 'numeric',
num_bathroom = 'numeric',
num_bedroom = 'numeric',
area_live_finished = 'numeric',
flag_tub_or_spa = 'numeric',
loc_latitude = 'numeric',
loc_longitude = 'numeric',
area_lot = 'numeric',
factor = 'factor',
loc_zip  = 'numeric',
loc_county = 'factor',
age = 'numeric',
flag_fireplace = 'numeric',
num_tax_building = 'numeric',
num_tax_total = 'numeric',
num_tax_land = 'numeric',
num_unit = 'numeric',
quality_factor = 'factor',
heating_factor = 'factor',
prop_living = 'numeric_character',
build_land_prop = 'numeric_character')
# make conversions
for (i in colnames(house_only16_mv)) {
if (colClasses[i][[1]] == 'numeric') {
house_only16_mv[[i]] <- as.numeric(house_only16_mv[[i]])
} else if (colClasses[[i]] == 'factor') {
house_only16_mv[[i]] <- as.factor(house_only16_mv[[i]])
} else if (colClasses[[i]] == 'numeric_character'){
house_only16_mv[[i]] <- as.numeric(sub(",", ".", house_only16_mv[[i]], fixed = TRUE))
}
}
str(house_only16_mv)
### PART 1: DATA PREPROCESSING ###--------------------------------------
# select the dataframe
data = na.omit(house_only16_mv)
# use only first 10'000
#data = data[1:10000,]
##set the seed to make your partition reproducible
set.seed(123)
smp_size <- floor(0.75 * nrow(data)) ## 75% of the sample size
train_ind <- sample(seq_len(nrow(data)), size = smp_size)
# features we want to omit for the model
omit <- c('id_parcel', 'loc_latitude', 'loc_longitude', 'loc_zip', 'loc_county', 'num_tax_building', 'num_tax_land', 'factor', 'build_land_prop')
# Split the data into train and test
train16 <- data[train_ind,]
test16 <- data[-train_ind, ]
# define training label = dependent variable
output_vector = as.matrix(log(train16[,'num_tax_total']))
test_vector = as.matrix(log(test16[,'num_tax_total']))
#omit variables and convert to numeric again
train16 <- train16 %>% select(-omit)
#train16 <- data.frame(sapply(train16, as.numeric))
test16 <- test16 %>% select(-omit)
#test16 <- data.frame(sapply(test16, as.numeric))
# convert categorical factor into dummy variables using one-hot encoding
sparse_matrix_train <- sparse.model.matrix(log(num_tax_total)~.-1, data = train16)
sparse_matrix_test <- sparse.model.matrix(log(num_tax_total)~.-1, data = test16)
# check the dimnames crated by the one-hot encoding
sparse_matrix_train@Dimnames[[2]]
# Create a dense matrix
dtrain <- xgb.DMatrix(data = sparse_matrix_train, label = output_vector)
dtest <- xgb.DMatrix(data = sparse_matrix_test, label=test_vector)
### PART 2: XGBOOST Training ###  -----------------------------
# # Model 1: Default parameters  -----------------------------
# #Let's start with a standard model and parameters and start optimizing the parameters later from here
#
# params <- list(booster = "gbtree",
#                objective = "reg:squarederror",
#                eta=0.3, # learning rate, between 0 and 1
#                gamma=0, # regularization (prevents overfitting), higher means more penality for large coef
#                max_depth=6, # max depth of trees, the more deep the more complex and overfitting
#                min_child_weight=1, # min number of instances per child node, blocks potential feature interaction and thus overfitting
#                subsample=1, # number of observations per tree, typically between 0.5 - 0.8
#                colsample_bytree=1) # number of variables per tree, typically between 0.5 - 0.9
#
# # using cross-validation to find optimal nrounds parameter
# xgbcv <- xgb.cv(params = params,
#                 data = dtrain,
#                 nrounds = 100,
#                 nfold = 10,
#                 showsd = T, # whether to show standard deviation of cv
#                 stratified = F,
#                 print_every_n = 1,
#                 early_stopping_rounds = 20, # stop if we don't see much improvement
#                 maximize = F,
#                 verbose = 2)
#
# # Result of best iteration
# xgbcv$best_iteration
#
# # first training with optimized nround
# xgb1 <- xgb.train(params = params,
#                   data = dtrain,
#                   nrounds = xgbcv$best_iteration,
#                   watchlist = list(test = dtest, train = dtrain),
#                   early_stopping_rounds = 20,
#                   maximize = F,
#                   eval_metric = "rmse"
# )
#
# # model prediction
# xgb1_pred <- predict(xgb1, dtest)
# rmse_xgb1 <- sqrt(mean((xgb1_pred - test_vector)^2))
# r2_xgb1 <- 1 - sum((test_vector-xgb1_pred)^2) / sum((test_vector-mean(xgb1_pred))^2)
# Find Optimized parameters  -----------------------------
set.seed(123)
fact_col <- colnames(train16)[sapply(train16,is.character)]
for(i in fact_col) set(train16,j=i,value = factor(train16[[i]]))
for (i in fact_col) set(test16,j=i,value = factor(test16[[i]]))
# create tasks for learner
traintask <- makeRegrTask(data = train16, target = 'num_tax_total')
testtask <- makeRegrTask(data = test16, target = 'num_tax_total')
# create dummy features, as classif.xgboost does not support factors
traintask <- createDummyFeatures(obj = traintask)
testtask <- createDummyFeatures(obj = traintask)
# create learner
# fix number of rounds and eta
lrn <- makeLearner("regr.xgboost", predict.type = "response")
lrn$par.vals <- list(objective="reg:squarederror",
eval_metric="rmse",
nrounds=100L,
eta = 0.1)
# set parameter space
params <- makeParamSet(makeDiscreteParam("booster", values = c("gbtree", "dart")),
makeIntegerParam("max_depth",lower = 3L,upper = 10L))
#makeNumericParam("min_child_weight",lower = 1L,upper = 10L),
#makeNumericParam("subsample",lower = 0.2,upper = 1),
# makeNumericParam("colsample_bytree",lower = 0.1,upper = 1),
# makeDiscreteParam("eta", values = c(0.05,0.1,0.2)))
# set resampling strategy
# If you have many classes for a classification type predictive modeling problem or the classes are imbalanced
#(there are a lot more instances for one class than another), it can be a good idea to create stratified folds when performing cross validation.
rdesc <- makeResampleDesc("CV",stratify = F, iters=5L)
# search strategy
# instead of a grid search we use a random search strategy to find the best parameters
ctrl <- makeTuneControlRandom(maxit = 10L)
# set parallel backend
parallelStartSocket(cpus = detectCores())
# parameter tuning
mytune <- tuneParams(learner = lrn,
task = traintask,
resampling = rdesc,
par.set = params,
control = ctrl,
show.info = T)
